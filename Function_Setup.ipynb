{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["This notebook contains the code to create `text_mining_project_utils.py`, which contains all the functions that would clutter the notebooks (like plots), or that are called in multiple notebooks.\n","\n","Notebooks `2_Topic_Modelling.ipynb`, `3_Summarization.ipynb` and `4_Topic_Modelling_on_Summaries.ipynb` all use some of the functions, which are imported at the start as: \"`from text_mining_project_utils import (**functions)`\""],"metadata":{"id":"-OTcONi_0KJY"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Rf1hMbkVXPej","executionInfo":{"status":"ok","timestamp":1768153146718,"user_tz":-60,"elapsed":19316,"user":{"displayName":"Francesco Volpi Ghirardini","userId":"04183541609253486658"}},"outputId":"c7d19f59-3335-4232-c549-2d83ccae5a0d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# Set the base path if needed\n","base_path = '/content/drive/My Drive'"],"metadata":{"id":"_1aUDH3oXSFU"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OSdXsHwpR7xE"},"outputs":[],"source":["with open(f'{base_path}/Text_Mining_Project_881765_933735/text_mining_project_utils.py', \"w\") as f:\n","\n","  f.write('''\n","\n","from sklearn.metrics.pairwise import cosine_similarity\n","import numpy as np\n","import itertools\n","import math\n","import matplotlib.pyplot as plt\n","from matplotlib.lines import Line2D\n","from tqdm.notebook import tqdm\n","\n","# ==========================================================================\n","# Functions to set up the variables used implicitly in the functions\n","# ==========================================================================\n","\n","def set_feature_names(x):\n","  global feature_names\n","  feature_names = x\n","\n","def set_dictionary(x):\n","  global dictionary\n","  dictionary = x\n","\n","def set_texts(x):\n","  global texts\n","  texts = x\n","\n","def set_top_words(x):\n","  global top_n\n","  top_n = x\n","\n","def set_vectorizer(x):\n","  global vectorizer\n","  vectorizer = x\n","\n","\n","\n","# ==========================================================================\n","# Functions to compute the metrics\n","# ==========================================================================\n","\n","def get_topics(model, bert=False):\n","  \"\"\"\n","  Extracts topics from a topic model.\n","  Args:\n","      model (any): The model.\n","      bert (bool, optional): Whether the model is from sklearn or a BERTopic model. Defaults to False.\n","  Returns:\n","      list: A list with the topics.\n","  Notes:\n","      - This function uses the global variables `feature_names` and `top_n`,\n","        make sure they are set in the notebook before calling this function.\n","  \"\"\"\n","  topics = []\n","  if not bert:\n","    for topic_weights in model.components_:\n","        top_features = [feature_names[i] for i in topic_weights.argsort()[-top_n:]]\n","        topics.append(top_features)\n","  else:\n","    for topic_id, topic in model.get_topics().items():\n","        if topic_id != -1:\n","          topics.append([word for word, _ in topic[:top_n]])\n","\n","  return topics\n","\n","\n","\n","def get_diversity(model, bert=False):\n","  \"\"\"\n","  Computes the diversity score for a topic model.\n","  Args:\n","      model (any): The model\n","      bert (bool, optional): Whether the model is from sklearn or a BERTopic model. Defaults to False.\n","  Returns:\n","      float: The diversity score.\n","  Notes:\n","    - This function uses the global variable `vectorizer`,\n","      make sure it is set in the notebook before calling this function.\n","  \"\"\"\n","  topics = get_topics(model, bert=bert)\n","  topics = [vectorizer.transform([' '.join(word for word in topic)]).toarray() for topic in topics]\n","  cos_sim = cosine_similarity(np.concatenate(topics, axis=0))[~np.eye(len(topics), dtype=bool)]\n","\n","  return 1 - np.mean(cos_sim)\n","\n","\n","\n","def get_coherence(model, bert=False):\n","  \"\"\"\n","  Computes the coherence score for a topic model.\n","  Args:\n","      model (any): The model.\n","      bert (bool, optional): Whether the model is from sklearn or a BERTopic model. Defaults to False.\n","  Returns:\n","      float: The coherence score.\n","  Notes:\n","    - This function uses the global variables`texts` and `dictionary`,\n","      make sure they are set in the notebook before calling this function.\n","  \"\"\"\n","  from gensim.models.coherencemodel import CoherenceModel\n","  topics = get_topics(model) if not bert else get_topics(model, bert=True)\n","  coherence_model = CoherenceModel(topics = topics,\n","                                   texts = texts,\n","                                   dictionary = dictionary,\n","                                   coherence = 'c_v'\n","                                   )\n","\n","  return coherence_model.get_coherence()\n","\n","\n","\n","def get_rouge(y_test, predicted):\n","  \"\"\"\n","  Computes F1 scores for ROUGE-1, ROUGE-2, ROUGE-L for a summarized text.\n","  Args:\n","      y_test (string): The real summary.\n","      predicted (string): The summarized doc.\n","  Returns:\n","      tuple: A tuple of three floats (rouge1_f1, rouge2_f1, rougeL_f1).\n","  \"\"\"\n","  from rouge_score import rouge_scorer\n","  scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer = True)\n","  scores = scorer.score(y_test, predicted)\n","\n","  return scores['rouge1'][2], scores['rouge2'][2], scores['rougeL'][2]\n","\n","\n","\n","# ==========================================================================\n","# Functions for the tuning process\n","# ==========================================================================\n","\n","def create_space(param_grid):\n","  \"\"\"\n","  Creates a parameter space from a parameter grid.\n","  Args:\n","      param_grid (dict): Dictionary mapping parameter names (str) to lists of possible values.\n","  Yields:\n","      dict: A dictionary representing one combination of parameters from the grid.\n","  \"\"\"\n","  keys = param_grid.keys()\n","  for values in itertools.product(*param_grid.values()):\n","    yield dict(zip(keys, values))\n","\n","\n","\n","def tuning_tester(model, param_grid, data):\n","  \"\"\"\n","  Evaluates a sklearn model on all hyperparameter combinations and computes metrics over multiple seeds.\n","  Args:\n","      model (any): The model.\n","      param_grid (dict): Dictionary mapping parameter names (str) to lists of possible values.\n","      data (any): Dataset to fit the model on (format depends on the model).\n","  Returns:\n","      dict: A dictionary containing lists of parameter values and corresponding evaluation metrics:\n","                  - Keys from `param_grid` contain the specific hyperparameter values for each combination\n","                  - \"diversity\": Mean diversity score over seeds\n","                  - \"diversity_std\": Standard deviation of diversity over seeds\n","                  - \"coherence\": Mean coherence score over seeds\n","                  - \"coherence_std\": Standard deviation of coherence over seeds\n","  \"\"\"\n","  results = {x:[] for x in param_grid.keys()}\n","  results.update({x:[] for x in [\"diversity\", \"diversity_std\", \"coherence\", \"coherence_std\"]})\n","\n","  for params in tqdm(create_space(param_grid), total=math.prod(map(len, param_grid.values())), desc='Evaluating model'):\n","    diversity, coherence = [], []\n","\n","    for seed in tqdm(range(3), leave=False, desc='Evaluating seed'):\n","      model.set_params(**params, random_state=seed).fit(data)\n","      diversity.append(get_diversity(model))\n","      coherence.append(get_coherence(model))\n","\n","    for key, value in params.items(): results[key].append(value)\n","    results[\"diversity\"].append(np.mean(diversity))\n","    results[\"diversity_std\"].append(np.std(diversity))\n","    results[\"coherence\"].append(np.mean(coherence))\n","    results[\"coherence_std\"].append(np.std(coherence))\n","\n","  return results\n","\n","\n","\n","def set_params(model, param_grid, best_iter):\n","  \"\"\"\n","  Sets the hyperparameters of a sklearn model to the best combination based on a given index.\n","  Args:\n","      model (any): The model.\n","      param_grid (dict): Dictionary mapping parameter names (str) to lists of possible values.\n","      best_iter (int): The index of the best combination.\n","  Returns:\n","      any: The same model instance with updated hyperparameters and random_state fixed to 15.\n","  \"\"\"\n","  best_params = {}\n","  for key, value in param_grid.items():\n","    best_params[key] = value[best_iter]\n","  model.set_params(**best_params, random_state=15)\n","  return model\n","\n","\n","\n","# ==========================================================================\n","# Functions to plot the results\n","# ==========================================================================\n","\n","def plot_top_words(model, title):\n","  \"\"\"\n","  Plots the top words for 10 topics from a fitted sklearn topic model.\n","  Args:\n","      model (any): The fitted sklearn topic model.\n","      title (str): Title for the entire figure.\n","  Returns:\n","      None: Displays a matplotlib figure.\n","  Notes:\n","      - This function uses the global variables `feature_names` and `top_n`,\n","        make sure they are set in the notebook before calling this function.\n","  \"\"\"\n","  fig, axes = plt.subplots(2, 5, figsize=(30, 15), sharex=True)\n","  axes = axes.flatten()\n","\n","  for topic_idx, topic in enumerate(model.components_[:10]):\n","      top_features_ind = topic.argsort()[-top_n:]\n","      top_features = feature_names[top_features_ind]\n","      weights = topic[top_features_ind]\n","\n","      ax = axes[topic_idx]\n","      ax.barh(top_features, weights, height=0.7)\n","      ax.set_title(f\"Topic {topic_idx +1}\", fontdict={\"fontsize\": 30})\n","      ax.tick_params(axis=\"both\", which=\"major\", labelsize=20)\n","      for i in \"top right left\".split():\n","          ax.spines[i].set_visible(False)\n","      fig.suptitle(title, fontsize=40)\n","\n","  plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n","  plt.show()\n","\n","\n","\n","def tuning_plotter(results):\n","  \"\"\"\n","  Plots coherence and diversity metrics with error bands from hyperparameter tuning results.\n","  Args:\n","      results (dict): Dictionary containing evaluation metrics for hyperparameter combinations.\n","  Returns:\n","      None: Displays a matplotlib figure.\n","  \"\"\"\n","  fig, axes = plt.subplots(2, 1, sharex=True)\n","  metric, color = ['coherence', 'diversity'], ['b', 'g']\n","\n","  for x in range(2):\n","    ax = axes[x]\n","    std_up = [results[metric[x]][j] + results[metric[x]+'_std'][j] for j in range(len(results[metric[x]]))]\n","    std_down = [results[metric[x]][j] - results[metric[x]+'_std'][j] for j in range(len(results[metric[x]]))]\n","    ax.plot(results[metric[x]], color=color[x], linewidth=1)\n","    ax.fill_between(range(len(results[metric[x]])), std_down, std_up, alpha=0.15, color = color[x])\n","    ax.set_xticks(range(len(results[metric[x]])))\n","    axes[1].set_xlabel(\"Hyperparameter combination\")\n","    ax.set_title(metric[x].title())\n","    ax.grid()\n","\n","  plt.show()\n","\n","\n","\n","def plot_metrics(models, labels):\n","  \"\"\"\n","  Plots coherence and diversity metrics for multiple models as a grouped bar chart.\n","  Args:\n","      models (list of tuples): Each tuple contains:\n","          - model: a fitted topic model\n","          - bert (bool, optional): Whether the model is from sklearn or a BERTopic model.\n","      labels (list of strings): Names of the models, used for the legend.\n","  Returns:\n","      None: Displays a matplotlib figure.\n","  \"\"\"\n","  metrics = ['Coherence', 'Diversity']\n","  coherences = [get_coherence(model, bert=idx) for model,idx in models]\n","  diversities = [get_diversity(model, bert=idx) for model,idx in models]\n","\n","  w = 0.1\n","  x= np.arange(len(metrics))*0.75\n","  fig, ax = plt.subplots()\n","  color = ['C0', 'C1', 'C2', 'C3']\n","  for i in range(len(labels)):\n","    ax.bar(x+i*w, [coherences[i], diversities[i]],\n","           width=w, label=labels[i], color=color[i])\n","\n","  ax.set_xticks(x + w * (4 - 1) / 2)\n","  ax.set_xticklabels(metrics)\n","  ax.set_ylabel('Score')\n","  ax.legend()\n","\n","  plt.show()\n","\n","\n","\n","def plot_rouge(summaries, labels, summary_labels):\n","  \"\"\"\n","  Plots F1 scores for ROUGE-1, ROUGE-2 and ROUGE-L with error bars for multiple summarizd docs.\n","  Args:\n","      summaries (list of list of str): a list of all the summaries for each method.\n","      labels (list of str): Names of the summarization methods.\n","      summary_labels (list of str): Real summaries.\n","  Returns:\n","      None: Displays a matplotlib figure.\n","  \"\"\"\n","  metrics = [\"ROUGE-1\", \"ROUGE-2\", \"ROUGE-L\"]\n","  results = {}\n","\n","  for i in range(len(labels)):\n","    results[labels[i]] = dict(zip(metrics, [[],[],[]]))\n","    for j in range(len(summary_labels)):\n","      score = get_rouge(summary_labels[j], summaries[i][j])\n","      for k in range(3): results[labels[i]][metrics[k]].append(score[k])\n","    for m in metrics: results[labels[i]][m] = np.mean(results[labels[i]][m])\n","\n","  w, x = 0.2, np.arange(len(metrics))\n","  fig, ax = plt.subplots()\n","  color = ['C9', 'C4', 'C8', 'C6']\n","  for i in range(4):\n","    ax.bar(x+i*w, [results[labels[i]][metric] for metric in metrics],\n","            width=w, label=labels[i], color=color[i])\n","\n","  color = ['red', 'green']\n","  for i in range(4,6):\n","    pos = 0\n","    for metric in metrics:\n","      ax.axhline(y=results[labels[i]][metric], xmin=pos, xmax=pos+1/3,\n","                color=color[i-4], linestyle=\"--\", linewidth=1)\n","      pos += 1/3\n","\n","  ax.set_xticks(x + w * (4 - 1) / 2)\n","  ax.set_xticklabels(metrics)\n","  ax.set_ylabel('Score')\n","  leg1 = ax.legend(bbox_to_anchor=(3/4, 1.0))\n","  ax.add_artist(leg1)\n","  line_handles = [\n","      Line2D([0], [0], color='green', linestyle='--', linewidth=1),\n","      Line2D([0], [0], color='red', linestyle='--', linewidth=1)\n","  ]\n","  ax.legend(line_handles, ['Best', 'Random'], loc='upper right')\n","\n","  plt.show()\n","\n","\n","\n","''')"]}]}